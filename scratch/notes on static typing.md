Some notes on static typing:  What it's about and why it's nice.

First, note that almost every language has some concept of what is, a priori, an unacceptable thing for a piece of code to say or do.  A few examples:

* Syntax:  If code is not well-formed, it is wrong.
* "null" pointers or object references:  If a program tries to read the memory address "0" or tries to send a message to a "nil" or "null" object, many languages will automatically throw an exception.
* Improper array indexing:  If x is an array, most languages consider x["hello there, array"] to be meaningless and will barf if you try it.
* Mentioning variables that aren't already declared:  Some languages require you to declare a variable before you can try to access it.

All these examples have one thing in common:  If the "bad" thing is done by code, it is taken by the language implementation as irrefutable evidence that the code is wrong.  The implementation will either signal that fact or just collapse on itself.

The essence of static typing is this:  If the implementation knows, a priori, that something is wrong or meaningless, it should do as much as possible to tell you so _before_ trying to execute the code.

Now, clearly, not everything that can go wrong can be automatically predicted before actually running the code.  If it could, we'd be able to solve the halting problem, which has been proven unsolvable.  But, all other things being equal, the more the compiler can tell you is wrong before you run the program the better.  Ultimately, a (static) type system is a logical system for analyzing whether or not the program contains one of these "fundamental" errors.  The usefulness of static typing varies with the strength of that logical system and the relevance of the taboo behaviors, and the user-friendliness of the system varies with the "automatedness" of the theorem prover in the implementation.

Under this looser interpretation, I would classify a lot of "dynamic" languages as (very weakly) statically typed.  For example, when you run a ruby or a python program the interpreter will tell you before it tries executing anything whether the syntax is valid.  Believe it or not, this is not possible in every language.  Perl is a classic example; certain constructs in the language cannot even be parsed unambiguously without knowing the value of certain variables _at that point in execution_.

With regard to static analysis, languages can be classified in (at least) two dimensions:  The strength of their logical system (i.e., how much it is possible to say about a program before running it) and the degree of automation in their program analysis.  When it comes to parsing, most languages are very strong in both dimensions:  It is possible to say unambiguously whether a program contains a syntax error without running it, and language implementations generally include completely automated tools for doing so, that require no "help" from the user to figure it out.

Other areas, though, vary wildly.  C and C++ have very weak logical systems which are not even sound (that is, it is possible to prove false things with them, such as that an arbitrary number is a function pointer), and require the user to hold the compiler's hands even through that.  Java has a slightly stronger system that is sound (arguably - it is quite possible to "prove" that an array holds a particular type of object when it doesn't, leading to a runtime error.  Whether the system is sound or not depends on whether that particular error is considered one that "ought" to be prevented[1]).  Java's "theorem prover" is just as manual as C and C++'s, though.

Typed functional languages such as ML and Haskell tend to sit and the other end of both spectrums (spectra?), because their type systems were designed from the start with this concept of types as formal logical systems in mind.  In Ruby the situation is also extreme, but back in the C/C++ direction.  The "everything is an object" mantra together with its very flexible message handling rules makes it so very little of what a program could do is actually considered "wrong" in the language in the first place (permissiveness vs. fail-fast error handling is a different, though related, issue).  That leaves mostly the very hard-to-predict "fundamental errors", such as memory exhaustion, etc., so in Ruby the logical system is almost trivial; if it's syntactically correct, then it's a valid program.

It seems Ruby would not gain a lot from a static type system, since everything is an object and every object can handle every message (even if just by shouting back at the caller).  That is, until you consider another use for static type systems (which most languages do not actually support very well at all): user-defined rules about what is and is not valid.  Once you start introducing powerful logical systems to a program, it is not much of a stretch to imagine using that logical system to enforce things _YOU_ want to be true of your program instead of just things some language inventor wanted to be true.  After all, they don't know what your program is even supposed to do, and you (hopefully!) do.

It is when a static type system reaches that level of sophistication that, in my opinion, static typing amply repays any and all "inconvenience" caused by overzealous compilers telling you your program is "wrong" without even trying to run it[2].  A powerful type system which lets me define _my own rules_ about what is and isn't allowed, and will tell me when I've broken _my own rules_, is truly an amazingly useful thing.




[1] Actually, this same line of reasoning can be applied to the C/C++ case, but if it is then the conclusion becomes that the type system is not actually claiming to statically identify _ANY_ kind of errors whatsoever.  So the type system is either unsound or trivial (which is almost the same thing - "unsound" refers to a situation where the statement "true" is provably equivalent to the statement "false".  "Trivial" refers to a situation where "true" and "false" are distinct, but no statement in the language is actually false except for "false" itself.  Either way, the system lacks the power to make any meaningful statements at all).

[2] Though, in my opinion, the opposite is far more inconvenient;  if my code is provably wrong, why should I have to run it and poke it and prod it to find that out?  Or even worse, why should any error that the compiler _could have caught_ actually be shipped to the customer just because I didn't manage to create a test case that actually tripped it?  Even with 100% code coverage in your test cases, there are still errors that a static analysis can catch that testing can easily miss.